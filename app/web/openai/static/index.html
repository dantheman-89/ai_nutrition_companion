<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Live AI Chat</title>
  <script src="https://cdn.tailwindcss.com"></script>
</head>
<body class="flex flex-col h-screen">
  <!-- messages container -->
  <div id="messages" class="flex-1 overflow-auto p-4 space-y-2 bg-gray-50"></div>

  <!-- input bar -->
  <div class="p-4 flex border-t">
    <input
      id="input"
      type="text"
      placeholder="Type a messageâ€¦"
      class="flex-1 border rounded px-2 py-1 mr-2"
    />
    <button
      id="send"
      class="bg-blue-500 text-white rounded px-4 py-1 mr-2"
    >
      Send
    </button>
    <button
      id="record"
      class="bg-indigo-500 text-white rounded px-4 py-1 flex items-center"
    >
      <span class="mr-2">ðŸŽ¤</span> Talk
    </button>
  </div>

  <!-- Status indicator -->
  <div id="status" class="p-2 text-xs text-gray-600 text-center"></div>

  <!-- Audio chunks debugging - hidden by default -->
  <div id="debug" class="p-2 bg-gray-100 text-xs text-gray-600 hidden"></div>

  <script>
    const messagesDiv = document.getElementById("messages");
    const inputEl = document.getElementById("input");
    const sendBtn = document.getElementById("send");
    const recordBtn = document.getElementById("record"); 
    const statusEl = document.getElementById("status");
    const debugEl = document.getElementById("debug");

    // Debug function
    const DEBUG = false;
    function debug(msg) {
      if (!DEBUG) return;
      console.log(msg);
      debugEl.classList.remove("hidden");
      debugEl.textContent += msg + "\n";
      if (debugEl.textContent.length > 1000) {
        debugEl.textContent = debugEl.textContent.slice(-1000);
      }
    }

    // Set status function
    function setStatus(message) {
      statusEl.textContent = message;
    }

    // Audio playback
    let audioContext;
    let audioQueue = [];
    let isPlaying = false;
    
    function getAudioContext() {
      if (!audioContext) {
        audioContext = new (window.AudioContext || window.webkitAudioContext)({
          sampleRate: 24000 // Match OpenAI's expected sample rate
        });
      }
      return audioContext;
    }
    
    // Convert base64 to ArrayBuffer
    function base64ToArrayBuffer(base64) {
      try {
        const binaryString = atob(base64);
        const bytes = new Uint8Array(binaryString.length);
        for (let i = 0; i < binaryString.length; i++) {
          bytes[i] = binaryString.charCodeAt(i);
        }
        return bytes.buffer;
      } catch (err) {
        console.error(`Base64 decoding error: ${err.message}`);
        return new ArrayBuffer(0);
      }
    }
    
    // Play audio chunk from base64
    async function playAudioChunk(base64Audio) {
      try {
        if (!base64Audio) return;
        
        // Add to queue and start playing if not already
        audioQueue.push(base64Audio);
        if (!isPlaying) {
          playNextAudioChunk();
        }
      } catch (err) {
        console.error(`Error queuing audio: ${err.message}`);
      }
    }
    
    // Play next audio chunk from queue
    async function playNextAudioChunk() {
      if (audioQueue.length === 0) {
        isPlaying = false;
        return;
      }
      
      isPlaying = true;
      const base64Audio = audioQueue.shift();
      
      try {
        const arrayBuffer = base64ToArrayBuffer(base64Audio);
        if (arrayBuffer.byteLength === 0) {
          playNextAudioChunk();
          return;
        }
        
        const ctx = getAudioContext();
        const audioBuffer = await ctx.decodeAudioData(arrayBuffer).catch(e => {
          console.error(`Audio decode error: ${e}`);
          return null;
        });
        
        if (!audioBuffer) {
          playNextAudioChunk();
          return;
        }
        
        const source = ctx.createBufferSource();
        source.buffer = audioBuffer;
        source.connect(ctx.destination);
        source.onended = playNextAudioChunk;
        source.start();
      } catch (err) {
        console.error(`Error playing audio: ${err.message}`);
        // Continue to next chunk even if there's an error
        playNextAudioChunk();
      }
    }

    // Audio recording variables
    let mediaRecorder;
    let isRecording = false;
    let audioStream;
    let audioProcessor;
    let sourceNode;
    let audioWorkletNode;
    let analyserNode;
    let scriptProcessorNode;

    // WebSocket connection
    let ws;
    let wsConnected = false;
    let lastAiElem = null;
    let waitingForResponse = false;
    let autoStopTimer = null;

    // Initialize WebSocket
    function initWebSocket() {
      // Update the WebSocket initialization to handle HTTPS
      const protocol = window.location.protocol === 'https:' ? 'wss:' : 'ws:';
      ws = new WebSocket(`${protocol}//${window.location.host}/ws`);
      
      ws.onopen = () => {
        wsConnected = true;
        setStatus("Connected");
      };
      
      ws.onclose = () => {
        wsConnected = false;
        setStatus("Disconnected");
      };
      
      ws.onmessage = async (e) => {
        try {
          const data = JSON.parse(e.data);
          
          if (data.type === "ping") {
            // Respond to server pings with pongs
            ws.send(JSON.stringify({ type: "pong" }));
            return;
          }
          
          if (data.type === "text_delta") {
            // Handle text deltas
            if (!lastAiElem) {
              // Create AI bubble placeholder if it doesn't exist
              const aiDiv = document.createElement("div");
              aiDiv.className = "self-start bg-gray-100 rounded p-2 max-w-xs";
              aiDiv.textContent = "AI: ";
              messagesDiv.appendChild(aiDiv);
              lastAiElem = aiDiv;
            }
            
            lastAiElem.textContent += data.content;
            messagesDiv.scrollTop = messagesDiv.scrollHeight;
          } 
          else if (data.type === "audio_chunk") {
            // Play audio chunks as they arrive
            playAudioChunk(data.audio);
          }
          else if (data.type === "done") {
            setStatus("Done");
            waitingForResponse = false;
            
            // If the user was recording, automatically stop recording as well
            if (isRecording) {
              stopRecording();
            }
          }
          else if (data.type === "error") {
            setStatus(`Error: ${data.message}`);
            waitingForResponse = false;
            
            if (isRecording) {
              stopRecording();
            }
          }
        } catch (err) {
          console.error(`Error handling message: ${err.message}`);
        }
        
        messagesDiv.scrollTop = messagesDiv.scrollHeight;
      };
    }

    // Send text message function
    function sendMessage() {
      const text = inputEl.value.trim();
      if (!text || waitingForResponse || !wsConnected) return;
      
      waitingForResponse = true;
      setStatus("Sending message...");
      
      // Initialize audio on first interaction
      getAudioContext();
      
      // User bubble
      const userDiv = document.createElement("div");
      userDiv.className = "self-end bg-blue-100 rounded p-2 max-w-xs";
      userDiv.textContent = "You: " + text;
      messagesDiv.appendChild(userDiv);

      // AI bubble placeholder
      const aiDiv = document.createElement("div");
      aiDiv.className = "self-start bg-gray-100 rounded p-2 max-w-xs";
      aiDiv.textContent = "AI: ";
      messagesDiv.appendChild(aiDiv);
      lastAiElem = aiDiv;

      // Send JSON over WebSocket
      ws.send(JSON.stringify({ type: "user_message", text }));
      inputEl.value = "";
      inputEl.focus();
      messagesDiv.scrollTop = messagesDiv.scrollHeight;
    }

    // Toggle recording function - simplified for server-side VAD
    async function toggleRecording() {
      if (isRecording) {
        stopRecording();
      } else {
        await startRecording();
      }
    }

    // Start audio recording with Web Audio API for PCM conversion
    async function startRecording() {
      if (isRecording || waitingForResponse || !wsConnected) return;

      try {
        // Initialize audio context
        const ctx = getAudioContext();
        
        // Request microphone access with specific constraints for best VAD performance
        audioStream = await navigator.mediaDevices.getUserMedia({
          audio: {
            channelCount: 1,          // Mono audio
            sampleRate: 24000,        // 24kHz sample rate to match server processing
            echoCancellation: true,   // Echo cancellation
            noiseSuppression: true,   // Noise suppression
            autoGainControl: true     // Auto gain control
          }
        });
        
        // Create source node from microphone
        sourceNode = ctx.createMediaStreamSource(audioStream);
        
        // Create analyser for visualization (optional)
        analyserNode = ctx.createAnalyser();
        analyserNode.fftSize = 2048;
        sourceNode.connect(analyserNode);
        
        // Check if AudioWorklet is supported (modern browsers)
        if (ctx.audioWorklet && !audioWorkletNode) {
          try {
            // Create a blob URL for our worklet code
            const workletCode = `
              class PCMProcessor extends AudioWorkletProcessor {
                constructor() {
                  super();
                  this.bufferSize = 2048; // Process in chunks
                  this.sampleRate = 24000; // Match OpenAI's rate
                }
                
                process(inputs, outputs) {
                  // Get mono input data
                  const input = inputs[0][0];
                  if (input && input.length > 0) {
                    // Send PCM data to main thread
                    this.port.postMessage({
                      pcmData: input
                    });
                  }
                  return true; // Keep processor alive
                }
              }
              
              registerProcessor('pcm-processor', PCMProcessor);
            `;
            
            const blob = new Blob([workletCode], { type: 'application/javascript' });
            const workletUrl = URL.createObjectURL(blob);
            
            // Load the worklet
            await ctx.audioWorklet.addModule(workletUrl);
            
            // Create the worklet node
            audioWorkletNode = new AudioWorkletNode(ctx, 'pcm-processor');
            
            // Handle PCM data from the worklet
            audioWorkletNode.port.onmessage = (event) => {
              if (event.data.pcmData && isRecording && wsConnected) {
                sendPCMDataToServer(event.data.pcmData);
              }
            };
            
            // Connect the nodes
            sourceNode.connect(audioWorkletNode);
            debug("Using AudioWorklet for PCM processing");
            
          } catch (err) {
            console.error("AudioWorklet failed, falling back to ScriptProcessor:", err);
            // Fall back to ScriptProcessor
            setupScriptProcessor(ctx, sourceNode);
          }
        } else {
          // Fall back to ScriptProcessor for older browsers
          setupScriptProcessor(ctx, sourceNode);
        }
        
        // Update UI
        recordBtn.classList.add('bg-red-500');
        recordBtn.classList.remove('bg-indigo-500');
        recordBtn.innerHTML = '<span class="mr-2">ðŸ”´</span> Listening...';
        isRecording = true;
        setStatus("Listening... (Speak normally and pause when done)");
        
        // Add user message bubble
        const userDiv = document.createElement("div");
        userDiv.className = "self-end bg-blue-100 rounded p-2 max-w-xs";
        userDiv.textContent = "You: ðŸŽ¤ Speaking...";
        messagesDiv.appendChild(userDiv);
        messagesDiv.scrollTop = messagesDiv.scrollHeight;
        
        // Tell server we're starting to record
        ws.send(JSON.stringify({ type: "toggle_recording", recording: true }));
        
        // Set auto-stop timer for safety (60 seconds max)
        clearTimeout(autoStopTimer);
        autoStopTimer = setTimeout(() => {
          if (isRecording) {
            debug("Auto-stopping recording after 60 seconds");
            stopRecording();
          }
        }, 60000);
        
      } catch (err) {
        console.error('Error starting recording:', err);
        setStatus(`Microphone error: ${err.message}`);
        
        if (audioStream) {
          audioStream.getTracks().forEach(track => track.stop());
        }
      }
    }
    
    // Set up ScriptProcessor as fallback for older browsers
    function setupScriptProcessor(ctx, source) {
      const bufferSize = 4096;
      scriptProcessorNode = ctx.createScriptProcessor(bufferSize, 1, 1);
      
      scriptProcessorNode.onaudioprocess = (e) => {
        if (isRecording && wsConnected) {
          // Get raw PCM data from input buffer
          const inputData = e.inputBuffer.getChannelData(0);
          sendPCMDataToServer(inputData);
        }
      };
      
      // Connect the nodes (must connect to destination to work)
      source.connect(scriptProcessorNode);
      scriptProcessorNode.connect(ctx.destination);
      debug("Using ScriptProcessor for PCM processing");
    }
    
    // Convert float32 PCM data to int16 and send to server
    function sendPCMDataToServer(float32Data) {
      try {
        // Create Int16Array (what OpenAI expects)
        const int16Data = new Int16Array(float32Data.length);
        
        // Convert Float32 [-1.0, 1.0] to Int16 [-32768, 32767]
        for (let i = 0; i < float32Data.length; i++) {
          const s = Math.max(-1, Math.min(1, float32Data[i]));
          int16Data[i] = s < 0 ? s * 0x8000 : s * 0x7FFF;
        }
        
        // Send the PCM data to server
        ws.send(int16Data.buffer);
      } catch (err) {
        debug(`Error sending PCM data: ${err.message}`);
      }
    }

    // Stop recording
    function stopRecording() {
      if (!isRecording) return;
      
      clearTimeout(autoStopTimer);
      
      // Update UI
      recordBtn.classList.add('bg-indigo-500');
      recordBtn.classList.remove('bg-red-500');
      recordBtn.innerHTML = '<span class="mr-2">ðŸŽ¤</span> Talk';
      isRecording = false;
      setStatus("Processing...");
      waitingForResponse = true;
      
      // Disconnect audio nodes
      if (sourceNode) {
        sourceNode.disconnect();
      }
      
      if (scriptProcessorNode) {
        scriptProcessorNode.disconnect();
      }
      
      if (audioWorkletNode) {
        audioWorkletNode.disconnect();
      }
      
      // Stop microphone
      if (audioStream) {
        audioStream.getTracks().forEach(track => track.stop());
      }
      
      // Tell server we've stopped recording
      if (wsConnected) {
        ws.send(JSON.stringify({ type: "toggle_recording", recording: false }));
      }
    }

    // Event listeners
    sendBtn.addEventListener("click", sendMessage);
    recordBtn.addEventListener("click", toggleRecording);
    
    inputEl.addEventListener("keydown", (e) => {
      if (e.key === "Enter") {
        e.preventDefault();
        sendMessage();
      }
    });
    
    // Initialize WebSocket and audio on page load
    document.addEventListener("DOMContentLoaded", () => {
      initWebSocket();
      setStatus("Ready");
      
      try {
        // Try to initialize audio context
        getAudioContext();
      } catch (e) {
        console.error(`Error initializing audio: ${e}`);
      }
    });
    
    // Keep audio context running
    document.body.addEventListener("click", () => {
      const ctx = getAudioContext();
      if (ctx && ctx.state !== 'running') {
        ctx.resume();
      }
    });
  </script>
</body>
</html>
